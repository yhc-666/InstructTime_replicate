"""
Dataset utilities for IHM/Pheno autoregressive and SFT training.
This dataset reads the PKL files generated by ``preprocess_tokenize.py`` and
constructs the prompt used by the language model.
"""

import pickle
from typing import List, Union

import torch
from torch.utils.data import Dataset
from multimodel import MultiTokenizer


class MultiDataset(Dataset):
    """Dataset for ICU autoregressive/SFT training.

    Parameters
    ----------
    pkl_files: str or list of str
        Path(s) to tokenised PKL dataset. When multiple paths are provided the
        samples are concatenated. This is used for autoregressive training where
        both IHM and Pheno data are combined.
    tokenizer: :class:`~multimodel.MultiTokenizer`
        Tokenizer used for both text and time-series tokens.
    mode: str
        ``"train"`` or ``"test"``. In ``train`` mode labels are appended to the
        prompt so that the model learns to autoregressively predict them.
    encoder_max_length: int, optional
        Maximum length after padding/truncation.
    model_id: int, optional
        Index of time-series tokenizer in ``MultiTokenizer``. Defaults to ``0``
        because IHM/Pheno share the same tokenizer.
    loss_type: {"ar", "sft"}, optional
        Loss computation strategy used in ``train`` mode. ``"ar"`` trains
        autoregressively using all tokens. ``"sft"`` (default) only uses the
        answer segment for loss.
    """

    def __init__(
        self,
        pkl_files: Union[str, List[str]],
        tokenizer: MultiTokenizer,
        mode: str,
        encoder_max_length: int = 512,
        model_id: int = 0,
        loss_type: str = "sft",
    ) -> None:
        assert mode in {"train", "test"}
        assert loss_type in {"ar", "sft"}
        super().__init__()

        if isinstance(pkl_files, str):
            pkl_files = [pkl_files]

        self.samples = []
        for path in pkl_files:
            with open(path, "rb") as f:
                data = pickle.load(f)
            task = "pheno" if "pheno" in path.lower() else "ihm"
            for item in data:
                self.samples.append(
                    {
                        "ts_ids": item["ts_ids"],
                        "notes": item["notes"],
                        "label": item["label"],
                        "task": task,
                    }
                )

        self.tokenizer = tokenizer
        self.mode = mode
        self.max_length = encoder_max_length
        self.model_id = model_id
        self.loss_type = loss_type

        # constant tokens
        self.bet_id = self.tokenizer.encode("<BET>")[0]
        self.eet_id = self.tokenizer.encode("<EET>")[0]
        self.offset = self.tokenizer.offsets[model_id]

    def __len__(self) -> int:  # type: ignore[override]
        return len(self.samples)

    def __getitem__(self, idx: int):  # type: ignore[override]
        sample = self.samples[idx]
        notes_text = "\n".join(sample["notes"])

        if sample["task"] == "ihm":
            instruction = (
                "Predict whether the patient will die in hospital "
                "given the 48-hour ICU multivariate vital-sign sequence and "
                "the following clinical notes."
            )
            label_text = "DECEASED" if int(sample["label"]) == 1 else "ALIVE"
        else:
            instruction = (
                "Predict the phenotype labels for the patient given the "
                "24-hour ICU multivariate vital-sign sequence and the "
                "following clinical notes."
            )
            pos = [i for i, v in enumerate(sample["label"]) if int(v) == 1]
            label_text = str(pos)

        prompt_text = f"{instruction}\nNotes:\n{notes_text}\n\nICU signals: "
        prompt_ids = self.tokenizer.encode(prompt_text)

        ts_ids = [tid + self.offset for tid in sample["ts_ids"]]
        base_ids = prompt_ids + [self.bet_id] + ts_ids + [self.eet_id]

        label_ids = self.tokenizer.encode(" " + label_text)
        eos_id = self.tokenizer.eos_token_id

        if self.mode == "train":
            base_input = base_ids + label_ids
            if self.loss_type == "ar":
                input_ids = base_input + [eos_id]
                labels = input_ids.copy()
            else:  # sft
                input_ids = base_input + [eos_id]
                labels = [-100] * len(base_ids) + label_ids + [eos_id]
        else:
            input_ids = base_ids + [eos_id]
            labels = label_text

        attn_masks = [1] * len(input_ids)
        input_ids, attn_masks = self._padding(input_ids, attn_masks)

        if self.mode == "train":
            labels, _ = self._padding(labels, attn_masks)
            return {
                "input_ids": torch.LongTensor(input_ids),
                "attn_masks": torch.FloatTensor(attn_masks),
                "label_ids": torch.LongTensor(labels),
            }
        else:
            return {
                "input_ids": torch.LongTensor(input_ids),
                "attn_masks": torch.FloatTensor(attn_masks),
                "label": labels,
            }

    def _padding(self, ids: List[int], masks: List[int]):
        """Pad sequences to ``self.max_length``."""
        ids = ids[: self.max_length]
        masks = masks[: self.max_length]

        pad_len = self.max_length - len(ids)
        if self.mode == "train":
            ids = ids + [self.tokenizer.pad_token_id] * pad_len
            masks = masks + [0] * pad_len
        else:
            ids = [self.tokenizer.pad_token_id] * pad_len + ids
            masks = [0] * pad_len + masks
        return ids, masks
